{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN TRAINING\n",
    "This notebook details the code that trains the CNNs described in [1]. These CNNs are trained on simulated gravitational lenses, up to 4 Euclid bands can be used as in put (H, Y, J, VIS). Once the CNN has been trained for the desired number of epochs, the model is updated with the weights that achieved the lowest validation loss during training. This trained CNN model is put into Eval mode and the test data is fed through the CNN. The model outputs on the test data is saved and compare to the truth table. The false positives, false negatives, true positives, and true negatives are saved. The Area Under the ROC Curve (AUC) is calculated and saved. \n",
    "\n",
    "\n",
    "\n",
    "The training process is saved along with any weights that generate a new minimum validation loss. The  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import glob\n",
    "import torch.utils.data as data_utils\n",
    "import math \n",
    "import time\n",
    "import datetime\n",
    "from astropy.io import fits\n",
    "from sklearn import metrics\n",
    "from Load_Images import J_LoadImages, Y_LoadImages, H_LoadImages, JYH_LoadImages, VIS_LoadImages, OU66_LoadImages, OU200_LoadImages\n",
    "from CNN_Networks import J_CNN, Y_CNN, H_CNN, JYH_CNN, VIS_CNN, OU66_CNN, OU200_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSTANTS \n",
    "This section describes the constants that can be changed in this code (aside from specification of the desired CNN). \n",
    "NNNumbers is set to 0 as this is the number of new minimum validation losses created during training of the CNN.\n",
    "\n",
    "Parameters of the CNN are defined her lr is the Learning Rate of the CNN. Threshold is the threshold for deciding what class each output is predicting. batch_size is the batch size of the CNN. This code will only accept whole batches, any batches that have less images than batch_size will be ignored. epochs is the amount of times the CNN sees the entire dataset during training. \n",
    "device specifies if the CNN will be trained on a cpu and a gpu if there is one avaliable.\n",
    "\n",
    "The amount of images used in this notebook are specified by TrainImages, ValidationImages, and TestImages. \n",
    "\n",
    "Band and date alter the filename of all the files saved by this notebook to indivialise each run of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NNNumbers = 0\n",
    "lr = 3e-4 \n",
    "Threshold = 0.5\n",
    "batch_size = 500\n",
    "epochs = 150\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TrainImages = 45000\n",
    "ValidationImages = 3000\n",
    "TestImages = 12000\n",
    "\n",
    "Band = 'JYH'\n",
    "date = 'Sept_21'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data \n",
    "There exists a CSV file that contains the Euclid ID, the truth value, and many other parameters about the image. The truth value was futher clarified so that any lens image with a n_source_im > 0, mag_eff > 1.6, and n_pix_source > 20 is defined as a lens and any other lenses are defined as a non-lens.\n",
    "This function outputs pairs of Euclid IDs and their truth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(Number):\n",
    "    \n",
    "    df = pd.read_csv('EuclidDataFile2.0train.csv')\n",
    "    n_source_im = df['n_source_im'].values\n",
    "    mag_eff = df['mag_eff'].values\n",
    "    n_pix_source = df['n_pix_source'].values\n",
    "    ID = df['Euclid ID'].values\n",
    "    \n",
    "    Output = np.zeros((Number,2))\n",
    "    \n",
    "    for i in range(Number):\n",
    "        if n_source_im[i] > 0:\n",
    "            if mag_eff[i] > 1.6:\n",
    "                if n_pix_source[i] > 20:\n",
    "                Output[i,:] = [int(ID[i]), 1]\n",
    "                else:\n",
    "                Output[i,:] = [int(ID[i]), 0]\n",
    "            else:\n",
    "                Output[i,:] = [int(ID[i]), 0]\n",
    "        else:\n",
    "                Output[i,:] = [int(ID[i]), 0]\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images\n",
    "Specify here which one of the CNNs you wish to train. This will create two outputs: Images, which is a numpy array of dimensions [1,bands, width, height] and NewOutput, a numpy array of pairs of values which are Euclid ID and the truth value. The Images width and height are 66 pixels for any CNN that uses just NISP bands, if the CNN uses the VIS band the width and height are 200 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images, NewOutput = J_LoadImages(Output)\n",
    "Images, NewOutput = Y_LoadImages(Output)\n",
    "Images, NewOutput = H_LoadImages(Output)\n",
    "Images, NewOutput = JYH_LoadImages(Output)\n",
    "Images, NewOutput = VIS_LoadImages(Output)\n",
    "Images, NewOutput = OU66_LoadImages(Output)\n",
    "Images, NewOutput = OU200_LoadImages(Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data For CNN\n",
    "\n",
    "The data are moved to a GPU if one is avaliable. The Image data and the truth table are converted from numpy arrays into PyTorch tensors. The image data and the truth table are split into the training, validation, and test sets. If the size of the TensorDataset is not exactly divisible by the batch size the remainders are removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = NewOutput\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Images = torch.from_numpy(Images).float().to(device)\n",
    "Output = torch.from_numpy(Output).type(torch.LongTensor).to(device)\n",
    "Output.type(torch.LongTensor) \n",
    "intiallabels = Output[:,1]\n",
    "labels = intiallabels\n",
    "\n",
    "train = data_utils.TensorDataset(Images[:TrainImages,:], intiallabels[:TrainImages])\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "validate = data_utils.TensorDataset(Images[TrainImages:TrainImages+ValidationImages,:], intiallabels[TrainImages:TrainImages+ValidationImages])\n",
    "validate_loader = data_utils.DataLoader(validate, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#validate_loader = train_loader\n",
    "\n",
    "test = data_utils.TensorDataset(Images[TrainImages+ValidationImages:TrainImages+ValidationImages+TestImages,:], intiallabels[TrainImages+ValidationImages:TrainImages+ValidationImages+TestImages])\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "classes = ['No Lens', 'Lens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CNN \n",
    "Here we load the CNN architectures that we want to use in this work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = J_CNN().to(device)\n",
    "model = Y_CNN().to(device)\n",
    "model = H_CNN().to(device)\n",
    "model = JYH_CNN().to(device)\n",
    "model = VIS_CNN().to(device)\n",
    "model = OU66_CNN().to(device)\n",
    "model = OU200_CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data \n",
    "Here we determine the amount of lenses and non-lenses in the training data. We use this to calculate a weighting for the Cross Entropy Loss to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lens = Output[:TrainImages,1].sum()\n",
    "no_nonLenses = len(Output[:TrainImages]) - no_lens\n",
    "weight_ten = torch.tensor([float(no_lens)/len(Output[:TrainImages]),float(no_nonLenses)/len(Output[:TrainImages])]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight_ten)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare For Training \n",
    "This prepares empty lists to store data generated during training about the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = epochs\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "timecount = []\n",
    "epochall = []\n",
    "valloss = []\n",
    "valacc = []\n",
    "trainloss = []\n",
    "trainacc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    epochStartTime = time.time()#####\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "    counter = 0.0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output, softmaxProbabilities = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        # Accuracy\n",
    "        max_scores, max_labels = output.data.max(1)\n",
    "        correct += (max_labels == target.data).sum()\n",
    "        counter += data.size(0)\n",
    "        train_acc = 100 * float(correct) / counter######\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "    counter = 0.0\n",
    "    for data, target in validate_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output, softmaxProbabilities = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "                # Accuracy\n",
    "        max_scores, max_labels = output.data.max(1)\n",
    "        correct += (max_labels == target.data).sum()\n",
    "        counter += data.size(0)\n",
    "        val_acc = 100 * float(correct) / counter######\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(validate_loader.dataset)\n",
    "    \n",
    "    epochFinishTime = time.time()\n",
    "    print('Epoch: {} tTraining Loss: {:.6f} tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    print('Epoch: {} tTraining Acc: {:.6f} tValidation Acc: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_acc,\n",
    "        val_acc\n",
    "        ))\n",
    "    print('Time: '+ str(epochFinishTime - epochStartTime))\n",
    "    \n",
    "    timecount.append(epochFinishTime - epochStartTime)\n",
    "    epochall.append(epoch+1)\n",
    "    trainloss.append(train_loss)\n",
    "    trainacc.append(train_acc)\n",
    "    valloss.append(valid_loss)\n",
    "    valacc.append(val_acc) \n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'Checkpoints/Pytorch1_model_'+str(date)+'_'+str(Band)+'_Explore_NN_BSGL2_'+str(NNNumbers)+'.pt')\n",
    "        torch.save(optimizer.state_dict(), 'Checkpoints/PyTorch1_Optim_'+str(date)+'_'+str(Band)+'_Explore_NN_BSGL2_'+str(NNNumbers)+'.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        NNNumbers = NNNumbers +1\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"Time\":timecount, \"Epoch\": epochall, \"Valloss\": valloss, \"Valacc\": valacc, \"trainloss\":trainloss, \"trainacc\":trainacc})\n",
    "df.to_csv('Data/Explore_NN_BSGL2_'+str(date)+'_'+str(Band)+'_TrainingData.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model Weights\n",
    "The set of weights which achieved the lowest validation loss are loaded into the CNN. This set of weights are used for use on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Checkpoints/Pytorch1_model_'+str(date)+'_'+str(Band)+'_Explore_NN_BSGL2_'+str(NNNumbers-1)+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "totalPred = []\n",
    "totalLabel = []\n",
    "totalSoftmaxProbabilities = []\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data = data.to(device)\n",
    "    target = target.type(torch.LongTensor) \n",
    "    target = target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output, softmaxProbabilities = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "        totalLabel.append(label.cpu().numpy())\n",
    "    totalPred.extend(pred.cpu().numpy())\n",
    "    totalSoftmaxProbabilities.extend(softmaxProbabilities.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test_loss/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate FP, TP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalLabel = np.array(totalLabel)\n",
    "totalPred = np.array(totalPred)\n",
    "totalSoftmaxProbabilities = np.array(totalSoftmaxProbabilities)\n",
    "\n",
    "lass = []\n",
    "for i in range(len(totalLabel)):\n",
    "    if totalSoftmaxProbabilities[i,0] < Threshold: # Classified as Non-Lens\n",
    "        if totalLabel[i] == 0: \n",
    "            lass.append('False Positive')\n",
    "        else:\n",
    "            lass.append('True Positive')\n",
    "    else:\n",
    "        if totalLabel[i] == 1: \n",
    "            lass.append('False Negative')\n",
    "        else:\n",
    "            lass.append('True Negative')\n",
    "lass = np.array(lass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save \n",
    "Here the CNN predictions for the test set are saved. The AUC is calculated for the test set and the data to plot this curve is saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Euclid ID\":Output[TrainImages+ValidationImages:TrainImages+ValidationImages+TestImages, 0].cpu(),\"Probability of Non-Lens\": totalSoftmaxProbabilities[:,0], \"Probability of Lens\": totalSoftmaxProbabilities[:,1], \"Classification\": lass,\"Target\":totalLabel})\n",
    "df.to_csv('Data/Explore_NN_BSGL2_'+str(date)+'_'+str(Band)+'_Results.csv', index=False)\n",
    "\n",
    "\n",
    "totalPred = np.array(totalPred)\n",
    "totalLabel = np.array(totalLabel)\n",
    "\n",
    "\n",
    "a, b, c = metrics.roc_curve(totalLabel,totalSoftmaxProbabilities[:,1]) # Probability of being a lens\n",
    "roc_auc = metrics.auc(a,b)\n",
    "\n",
    "df = pd.DataFrame({\"a\":a, \"b\":b})\n",
    "df.to_csv(\"Data/ROC_CURVE_Explore_NN_BSGL2_\"+str(date)+\"_\"+str(Band)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Detecting gravitational lenses using machine learning: exploring interpretability and sensitivity to rare lensing configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
